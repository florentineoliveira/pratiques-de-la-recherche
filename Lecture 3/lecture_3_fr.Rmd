---
title: "L'estimateur des Moindres Carrés Ordinaires (OLS)"
subtitle: "Pratiques de la Recherche en Économie"
author: "Florentine Oliveira"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ['metropolis', 'metropolis-fonts', 'custom.css']
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---


```{r setup, include = FALSE, warning = FALSE, message = FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# Load packages
library(tidyverse)
library(here)
library(pander)
library(ggthemes)
library(gapminder)
library(haven)
library(broom)
library(countdown)
library(xaringanthemer)
library(latex2exp)
library(xaringanExtra)
library(parallel)
library(gridExtra)

# countdown style
countdown(
  color_border              = "#dd0747",
  color_text                = "black",
  color_running_background  = "#dd0747",
  color_running_text        = "white",
  color_finished_background = "white",
  color_finished_text       = "#dd0747",
  color_finished_border     = "#dd0747",
  font_size = "1.5em" 
)


style_xaringan( 
  title_slide_background_color = "#dd0747",
  title_slide_text_color = "#eee",
  text_color = "black",
  header_color = "#dd0747",
  text_bold_color = "#dd0747",
  text_slide_number_color = "black",
  code_inline_background_color = "#e0e1df",
  base_font_size = "24px",
  text_font_family = "Palatino",
  header_font_family = "Palatino",
  header_h1_font_size = "45px",
  header_h2_font_size = "35px",
  header_h3_font_size = "28px",
  inverse_background_color = "#f8cdda",
  inverse_text_color = "black",
  inverse_header_color = "#dd0747")

# Theme with only x and y axis ans names
theme_minimum = theme_bw() + 
  theme(
  text = element_text(family = "Palatino"),
  line = element_blank(),  # Masquer toutes les lignes par défaut
  rect = element_blank(),  # Masquer tous les rectangles par défaut
  axis.line = element_line(color = "black"),  # Afficher les lignes des axes
  axis.ticks = element_blank(),  # Masquer les ticks des axes
  plot.title = element_blank(),  # Masquer le titre du graphique
  panel.background = element_blank(),  # Masquer le fond du panneau
  panel.grid.major = element_blank(),  # Masquer la grille principale
  panel.grid.minor = element_blank(),  # Masquer la grille secondaire
  plot.background = element_blank(),  # Masquer le fond du graphique
  legend.position = "none"  # Masquer la légende
)


theme_empty = theme_bw() + 
  theme(
  text = element_text(family = "Palatino"),
  line = element_blank(),  # Masquer toutes les lignes par défaut
  rect = element_blank(),  # Masquer tous les rectangles par défaut
  axis.line = element_line(color = "black"),  # Afficher les lignes des axes
  axis.ticks = element_blank(),  # Masquer les ticks des axes
  axis.text = element_blank(),  # Masquer les étiquettes des axes
  axis.title = element_text(size = 12),  # Afficher les titres des axes
  plot.title = element_blank(),  # Masquer le titre du graphique
  panel.background = element_rect(fill = "#FAFAFA", color = "#FAFAFA"),  # Masquer le fond du panneau
  panel.grid.major = element_blank(),  # Masquer la grille principale
  panel.grid.minor = element_blank(),  # Masquer la grille secondaire
  plot.background = element_rect(fill = "#FAFAFA", color = "#FAFAFA"),  # Masquer le fond du graphique
  legend.position = "none"  # Masquer la légende
)

#theme_empty = theme_bw() + 
#  theme(
#    text = element_text(family = "Palatino"),
#    line = element_blank(),  # Masquer toutes les lignes par défaut
#    rect = element_blank(),  # Masquer tous les rectangles par défaut, sauf le fond
#    axis.line = element_line(color = "black"),  # Afficher les lignes des axes
#    axis.ticks = element_blank(),  # Masquer les ticks des axes
#    axis.text = element_blank(),  # Masquer les étiquettes des axes
#    axis.title = element_text(size = 12),  # Afficher les titres des axes
#    plot.title = element_blank(),  # Masquer le titre du graphique
#    panel.background = element_rect(fill = "#FAFAFA", color = "#FAFAFA"),  # Fond du panneau #FAFAFA
#    panel.grid.major = element_blank(),  # Masquer la grille principale
#    panel.grid.minor = element_blank(),  # Masquer la grille secondaire
#    plot.background = element_rect(fill = "#FAFAFA", color = "#FAFAFA"),  # Fond du graphique #FAFAFA
#    legend.position = "none"  # Masquer la légende
#  )

# style_mono_accent(
#   base_color = "#dd0747",
#   header_font_google = google_font("Palatino"),
#   text_font_google   = google_font("Palatino", "300", "300i"),
#   code_font_google   = google_font("Palatino")
# )

```

```{R, gen dataset, include = F, cache = T}
set.seed(123)

# Population and sample size
n_population = 100
n_sample = 40

# True parameters
a = 1.6
b = 0.7

# Regressor and error term
x = rnorm(n_population, mean = 3, sd = 1.5)
e = rnorm(n_population, mean = 0, sd = 1)
  
# Outcome
y = a + b*x + e

# Dataframe
df = data.frame(
  a = rep(a, n_population),
  b = rep(b, n_population),
  x = x,
  e = e,
  y = y,
  s1 = sample(x = c(rep(TRUE, n_sample), rep(FALSE, n_population - n_sample))),
  s2 = sample(x = c(rep(TRUE, n_sample), rep(FALSE, n_population - n_sample))),
  s3 = sample(x = c(rep(TRUE, n_sample), rep(FALSE, n_population - n_sample)))
)

# Regressions
lm0 <- lm(y ~ x, data = df)
lm1 <- lm(y ~ x, data = filter(df, s1 == T))
lm2 <- lm(y ~ x, data = filter(df, s2 == T))
lm3 <- lm(y ~ x, data = filter(df, s3 == T))

```

layout: true

---
# Cette séance

&nbsp;

1. L'estimateur des MCO    
  1.1. Modèle linéaire univarié et interprétation géométrique    
  1.3. Du modèle univarié au modèle multivarié    
  
2. Hypothèses de l'estimateur des MCO   
  2.1. Linéarité   
  2.2.      
  
3. Propriétés statistiques   
  3.1. Espérance   
  3.2. Variance     
  
4. Remise en cause des hypothèses       


---
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique   

La régression linéaire simple est une méthode statistique permettant de trouver une relation **linéaire** entre

  * une **variable expliquée** (ou **variable dépendante** ou ***outcome***), *** $y$ ***
  
  * une **variable explicative** (ou **variable indépendante** ou **régresseur**), *** $x$ ***
  
  
La relation linéaire entre $y$ et $x$ n'est pas parfaite: elle est perturbée par une **erreur** (ou **bruit** ou ***noise***), $\varepsilon$.

Pour chaque individu $i$, on observe à la fois $x_i$ et $y_i$ ce qui nous permet de représenter la *distribution jointe* de ces deux variables.

Le modèle linéaire univarié s'écrit, $\forall \; i$, 

$$y_i = \alpha + \beta x_i + \varepsilon_i$$


---
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

On considère l'échantillon suivant

--

```{R, nuage, echo = F, dev = "svg", fig.width = 9, fig.height = 4.5}
ggplot(data = df, aes(x = x, y = y)) +
  geom_point(size = 3, color = "black", alpha = 0.9) +
  scale_x_continuous(limits = c(-1,8)) +
  scale_y_continuous(limits = c(0.5,7)) +
  theme_empty 

```


---
count: false
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

Pour toute droite $\tilde{y} = \tilde{\alpha} + \tilde{\beta} x$,

```{R, nuage et droite, echo = F, dev = "svg", fig.width = 9, fig.height = 4.5}

# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 4
b1 <- 0.4
# The plot
x_min2 <- sort(df$x)[2]
y_min2 <- df$y[df$x == x_min2]
y_hat_min2 <- y_hat(x_min2, b0, b1)

ggplot(data = df, aes(x = x, y = y)) +
    geom_point(size = 3, color = "black", alpha = 0.9) +
    geom_abline(intercept = b0, slope = b1, color = "#f08b27", size = 1, alpha = 0.9)  +
    annotate("text", x = -0.4, y = 3.2, label = "tilde(y)", parse = TRUE, color = "#f08b27", size = 6, family = "serif") +
    scale_x_continuous(limits = c(-1,8)) +
    scale_y_continuous(limits = c(0.5,7)) +
    theme_empty

```



---
count: false
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

Pour toute droite $\tilde{y} = \tilde{\alpha} + \tilde{\beta} x$, on peut calculer les erreurs: $e_i = y_i - \tilde{y}_i$

```{R, nuage et droite et point y2, echo = F, dev = "svg", fig.width = 9, fig.height = 4.5}

ggplot(data = df, aes(x = x, y = y)) +
  geom_point(size = 3, color = "black", alpha = 0.9) +
  geom_point(data = df %>% filter(x == x_min2), size = 3, color = "#f08b27", alpha = 0.9) +
  geom_abline(intercept = b0, slope = b1, color = "#f08b27", size = 1, alpha = 0.9) + 
  annotate("text", x = x_min2, y = y_min2 - 0.2, label = paste0("y[2]"), parse = TRUE, color = "#f08b27", size = 6, family = "serif") +
  annotate("text", x = -0.4, y = 3.2, label = "tilde(y)", parse = TRUE, color = "#f08b27", size = 6, family = "serif") +
  scale_x_continuous(limits = c(-1,8)) +
  scale_y_continuous(limits = c(0.5,7)) +
  theme_empty

```


---
count: false
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

Pour toute droite $\tilde{y} = \tilde{\alpha} + \tilde{\beta} x$, on peut calculer les erreurs: $e_i = y_i - \tilde{y}_i$

```{R, nuage et droite et point y2 et e2, echo = F, dev = "svg",  fig.width = 9, fig.height = 4.5}

ggplot(data = df, aes(x = x, y = y)) +
    geom_point(size = 3, color = "black", alpha = 0.9) +
    geom_point(data = df %>% filter(x == x_min2), size = 3, color = "#f08b27", alpha = 0.9) +
    geom_abline(intercept = b0, slope = b1, color = "#f08b27", size = 1, alpha = 0.9) +
    geom_segment(aes(x = x_min2, xend = x_min2, y = y_min2, yend = y_hat_min2), color = "#f08b27", size = 0.5, alpha = 0.2) +
    annotate("text", x = -0.4, y = 3.2, label = "tilde(y)", parse = TRUE, color = "#f08b27", size = 6, family = "Palatino") +
    annotate("text", x = x_min2, y = y_min2 - 0.2, label = paste0("y[2]"), parse = TRUE, color = "#f08b27", size = 6, family = "Palatino") +
    annotate("text", x = x_min2 - 0.1, y = (y_min2 + y_hat_min2) / 2, label = expression(epsilon[2]), color = "#f08b27", size = 6, family = "Palatino") +
    scale_x_continuous(limits = c(-1,8)) +
    scale_y_continuous(limits = c(0.5,7)) +
    theme_empty
```


---
count: false
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

Pour toute droite $\tilde{y} = \tilde{\alpha} + \tilde{\beta} x$, on peut calculer les erreurs: $e_i = y_i - \tilde{y}_i$

```{R, nuage et droite et toutes erreurs, echo = F, dev = "svg",  fig.width = 9, fig.height = 4.5}

ggplot(data = df, aes(x = x, y = y)) +
    geom_point(size = 3, color = "black", alpha = 0.9) +
    geom_abline(intercept = b0, slope = b1, color = "#f08b27", size = 1, alpha = 0.9) +
    geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
    annotate("text", x = -0.4, y = 3.2, label = "tilde(y)", parse = TRUE, color = "#f08b27", size = 6, family = "serif") +
    scale_x_continuous(limits = c(-1,8)) +
    scale_y_continuous(limits = c(0.5,7)) +
    theme_empty
```


---
count: false
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

Pour toute droite $\tilde{y} = \tilde{\alpha} + \tilde{\beta} x$, on peut calculer les erreurs: $e_i = y_i - \tilde{y}_i$

```{R, nuage et autre droite et toutes erreurs , echo = F, dev = "svg", fig.width = 9, fig.height = 4.5}

b0 <- 5
b1 <- -0.2

ggplot(data = df, aes(x = x, y = y)) +
    geom_point(size = 3, color = "black", alpha = 0.9) +
    geom_abline(intercept = b0, slope = b1, color = "#f08b27", size = 1, alpha = 0.9) +
    geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
    annotate("text", x = -0.4, y = 5.6, label = "tilde(y)", parse = TRUE, color = "#f08b27", size = 6, family = "Palatino") +
    scale_x_continuous(limits = c(-1,8)) +
    scale_y_continuous(limits = c(0.5,7)) +
    theme_empty
```



---
count: false
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

SCE = $\left(\sum \varepsilon_i^2\right)$: les erreurs importantes ont de plus grosses pénalités.

```{R, penalties, echo = F, dev = "svg", fig.width = 9, fig.height = 4.5}

df = df %>%
  mutate(y_hat = y_hat(x, b0, b1),
         distance = abs(y - y_hat))

ggplot(data = df, aes(x = x, y = y)) +
    annotate("text", x = -0.4, y = 5.6, label = "tilde(y)", parse = TRUE, color = "#f08b27", size = 6, family = "Palatino") +
    geom_rect(aes(xmin = x, xmax = x + distance, ymin = pmin(y, y_hat), ymax = pmax(y, y_hat)), 
              color = "#f7c593", fill = "#f7c593", alpha = 0.09) +
    geom_point(size = 3, color = "black", alpha = 0.9) +
    geom_abline(intercept = b0, slope = b1, color = "#f08b27", size = 1, alpha = 0.9) +
    geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
    scale_x_continuous(limits = c(-1,8)) +
    scale_y_continuous(limits = c(0.5,7)) +
    theme_empty
```


---
count: false
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

L'estimateur des MCO (*OLS*) calcule $\hat{\alpha}$ et $\hat{\beta}$ qui **<span style="color:#dd0747;">minimisent la SCE</span>**.

```{R, mco penalties, echo = F, dev = "svg", fig.width = 9, fig.height = 4.5}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]

# The plot
df = df %>%
  mutate(y_hat = y_hat(x, b0, b1),
         distance = abs(y - y_hat))

ggplot(data = df, aes(x = x, y = y)) +
    annotate("text", x = -0.4, y = 2, label = "hat(y)", parse = TRUE, color = "#dd0747", size = 6, family = "Palatino") +
    geom_rect(aes(xmin = x, xmax = x + distance, ymin = pmin(y, y_hat), ymax = pmax(y, y_hat)), 
              color = "#f7c593", fill = "#f7c593", alpha = 0.09) +
    geom_point(size = 3, color = "black", alpha = 0.9) +
    geom_abline(intercept = b0, slope = b1, color = "#dd0747", size = 1, alpha = 0.9) +
    geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +   
    scale_x_continuous(limits = c(-1,8)) +
    scale_y_continuous(limits = c(0.5,7)) +
    theme_empty 
```

---
count: false
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

L'estimateur des MCO (*OLS*) calcule $\hat{\alpha}$ et $\hat{\beta}$ qui **<span style="color:#dd0747;">minimisent la SCE</span>**.

```{R, mco, echo = F, dev = "svg",fig.width = 9, fig.height = 4.5}

ggplot(data = df, aes(x = x, y = y)) +
    annotate("text", x = -0.4, y = 2, label = "hat(y)", parse = TRUE, color = "#dd0747", size = 6, family = "Palatino") +
    geom_point(size = 3, color = "black", alpha = 0.9) +
    geom_abline(intercept = b0, slope = b1, color = "#dd0747", size = 1, alpha = 0.9) +
    geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
    scale_x_continuous(limits = c(-1,8)) +
    scale_y_continuous(limits = c(0.5,7)) +
    theme_empty 
```

---
name: OLS
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

### Formellement

L'estimateur des MCO calcule $\hat{\alpha}$ et $\hat{\beta}$ qui minimmise la Somme des Carrés des Erreurs (SCE, ou Sum of Squared Errors en anglais) : 

$$\min_{\hat{\alpha},\, \hat{\beta}} \text{SCE} =\sum_{i=1}^N \varepsilon _i^2$$
On obtient, dans le cas univarié:

.center[
$\hat{\alpha} = \overline{y} - \hat{\beta} \overline{x}$ 

$\hat{\beta} = \dfrac{Cov(x,y)}{Var(x)}$
]

[Maths](#derivation)

---
# 1. L'estimateur des MCO   

## 1.1. Modèle linéaire univarié et interprétation géométrique  

### Implémentation sur `R`

- Calcul de la variance empirique

```{r, echo = TRUE, eval = FALSE}
var(x)
```

- Calcul de la covariance empirique
```{r, echo = TRUE, eval = FALSE}
cov(x,y)
```

- Régression linéaire (simple)
```{r, echo = TRUE, eval = FALSE}
lm(variable dépendante ~  variable indépendante, data = data.frame)
```

- Résultats de l'estimation visibles avec la commande `summary`

---
# 2. Estimateur des Moindres Carrés Ordinaires

## $\hat{\beta}$ est une variable aléatoire
.pull-left[
```{R, scatter1, echo = F, fig.fullwidth = T, dev = "svg", fig.width = 6, fig.height = 5}
ggplot(data = df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#dd0747", size = 2) +
geom_point(color = "black", size = 4) +
theme_empty
```
]

--

.pull-left[
```{R, sample1 scatter, echo = F, fig.fullwidth = T, dev = "svg", fig.width = 6, fig.height = 5}
ggplot(data = df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#dd0747", size = 3, alpha = 0.3
) +
geom_point(aes(shape = s1), color = "black", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```
]


---
# 2. Estimateur des Moindres Carrés Ordinaires

## $\hat{\beta}$ est une variable aléatoire
.pull-left[

```{R, scatter1 opur sample 2, echo = F, fig.fullwidth = T, dev = "svg", fig.width = 6, fig.height = 5}
ggplot(data = df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#dd0747", size = 2) +
geom_point(color = "black", size = 4) +
theme_empty
```


]


.pull-left[

```{R, sample2 scatter, echo = F, fig.fullwidth = T, dev = "svg", fig.width = 6, fig.height = 5}
ggplot(data = df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#dd0747", size = 3, alpha = 0.3
) +
geom_point(aes(shape = s2), color = "black", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```
]



---
# 2. Estimateur des Moindres Carrés Ordinaires

## $\hat{\beta}$ est une variable aléatoire
.pull-left[

```{R, scatter1 opur sample 3, echo = F, fig.fullwidth = T, dev = "svg", fig.width = 6, fig.height = 5}
ggplot(data = df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#dd0747", size = 2) +
geom_point(color = "black", size = 4) +
theme_empty
```
]


.pull-left[

```{R, sample3 scatter, echo = F, fig.fullwidth = T, dev = "svg", fig.width = 6, fig.height = 5}
ggplot(data = df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#dd0747", size = 3, alpha = 0.3
) +
geom_point(aes(shape = s3), color = "black", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "darkgrey", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "darkgrey", alpha = 0.3
) +
geom_abline(
  intercept = lm3$coefficients[1], slope = lm3$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```
]

---
# 2. Estimateur des Moindres Carrés Ordinaires

## $\hat{\beta}$ est une variable aléatoire

.pull-left[
```{R, simulation, echo = F, dev = "png", dpi = 100, cache = T, fig.width = 6, fig.height = 5}
# Définir la fonction de régression pour les simulations
simulate_regression <- function(seed, df, n_sample) {
  set.seed(seed)
  sample_df <- df %>% sample_n(n_sample)
  model <- lm(y ~ x, data = sample_df)
  coefs <- coef(model)
  return(data.frame(intercept = coefs[1], slope = coefs[2]))
}

# Effectuer les simulations en parallèle
results <- do.call(rbind, mclapply(1:10000, simulate_regression, df = df, n_sample = n_sample, mc.cores = parallel::detectCores()))

# Tracer les résultats
ggplot() +
  geom_abline(data = results, aes(intercept = intercept, slope = slope), alpha = 0.01, color = "black") +
  geom_point(data = df, aes(x = x, y = y), size = 2, color = "black", alpha = 0.8) +
  geom_smooth(data = df, aes(x = x, y = y), method = "lm", color = "#dd0747", se = FALSE, size = 1) +
  theme_empty

```
]

--

.pull-right[

- En **moyenne**, les droites de régressions sur les échantillons sont très proches de la droite de régression sur l'ensemble de la population


- Mais certaines en sont très éloignées


- ** $\hat{\beta}$ est une variable aléatoire : sa valeur est propre à l'échantillon sur lequel il est estimé**


- $\implies$ Tout l'enjeu pour l'économètre est d'assurer que l'échantillon est aléatoire et/ou représentatif de telle sorte à ce que $\hat{\beta} \rightarrow \beta$

]

---
### Échantillon non représentatif

.pull-left[
```{R, selected sample, echo = F, dev = "png", dpi = 100, cache = T,  fig.width = 6, fig.height = 5}
selected_sample <- df %>% filter(y > quantile(y, 0.75)) %>% mutate(selected = "Selected")
df <- df %>% mutate(selected = ifelse(x %in% selected_sample$x & y %in% selected_sample$y, "Selected", "not selected"))

ggplot() +
    geom_point(data = df, aes(x = x, y = y, shape = selected), size = 3, color = "black", alpha = 0.8) +
    scale_shape_manual(values = c("Selected" = 19, "not selected" = 1)) +
    geom_smooth(data = selected_sample, aes(x = x, y = y), method = "lm", color = "#921539", se = FALSE, size = 1) +
    geom_smooth(data = df, aes(x = x, y = y), method = "lm", color = "#dd0747", se = FALSE, size = 1,  alpha = 0.5) +
    theme_empty
```
]


---
background-color: #f19bb5
# Application 

### Taille de la fratrie et performances scolaires

`r countdown(minutes = 7, top = 0)`

**Intuition** 

1: Selon vous, quelle est la relation entre **taille de la fratrie** et **performances scolaires** ?


**Sur `R`: se familiariser avec les données**

 
2: Importer la [base de données]() `simulated_data_black_et_al_2005.csv` que vous nommerez `data`

3: Calculer quelques statistiques descriptives de la **taille de la fratrie** (`family_size`) et du **nombre d'années d'études** (`education`): 
  - moyenne
  - écart-type
  - corrélation entre les deux variables

4: Représenter le nuage de points qui définit la relation entre **taille de la fratrie** et **nombre d'années d'études**


---
background-color: #fbe6ec
# Solution

### Intuition 

Selon vous, quelle est la relation entre **taille de la fratrie** et **performances scolaires** ?

--

- Relation théorique ([Becker (1960)](https://www.nber.org/system/files/chapters/c2387/c2387.pdf), [Becker and Lewis (1973)](https://www.jstor.org/stable/1840425), [Becker and Tomes (1976)](https://www.jstor.org/stable/1831106)): 

  - arbitrage entre la quantité et la *qualité* des enfants au sein d'une famille.

--

- Relations empiriques ([Black, Devereux and Salvanes (2005)](https://watermark.silverchair.com/120-2-669.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA2YwggNiBgkqhkiG9w0BBwagggNTMIIDTwIBADCCA0gGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMRUFpt9yUxU8Wuu4FAgEQgIIDGVcdvdpNZkmD_Fbj4_wa9HaGQTAoM2tQsF2wuRUmWvvlH23Fhxp68TwZUK_P2MpI1e4pA0PyMp-QAURQMsN_P7FcQuEoYqMkBs_7l-8reeiCLHbQYIdRAyJ7Ud-hDxSJwuZ1iqH-7kMcMtNHtrlh5JydP2Ya8p9GRWtFVmtfq4bzIbKfbqh2dI4_dv_8mVc-h8CQQxE-J7ME9RH9pw-1ZQccud0e4RCqHuVa6tR3ByEceFgsmwl1l779gZjQGoBrLm5sBLxc47Ni6bVW8_Czit3EPJjaq6vh22L8XnfsF256FHmtDgmSFUTljZ8l37VwGA4Y_4q62_k4RKJZbJft3i7_XW2eJxcfWAFTje1jjUIHi81WitYT24sisKkRmqHak6_yQFwbayocifHA23T8PfXoKqkLlDO9ewUYL83HrGsOrPHSd8JbitwP400KOWotEZVDATVqQxJhnJRa6LsVTtiYuZVcC6-PLcVSTAHPDbRF7Q_WcB4QL-0MeEorgQ-ULn_9trRFCjrs3tXlCo_6qPEvDFtAm3YAcZVVEaCcMRcwY5cnLeIL7q2gsB06txdyzGgriCrc_gMZksCy3M_V32yX8pP4RE81BP6cggKZu7BwlLzh3nOozg0AJqvC00IKcJ7k5iMhjSARFURevVtIhWz3RQ6_CAQAU5M4PfSaAnYzDHLO2FDKfnTThgJmZEYMAqUUJOXgLn4jrcE4OLXxuPLHfxdiqoAiAfhzTsTu6Wvn5x0YTaRq27_2bhw2pMXRNDotTjHQW6StxWxhlCvynLBVONRE4YKiXLN_YCXTd0FNjEyGQQRjmLbTQK6vvGgReEP2aUdB4-FarpRYU0b3DMFn6eFCUx0hdFMc2BunByvGDB8f3h5aubWaEZM4E-H-a7fIA_FE9wAORKqHd7wM1rx2x3nYVGk9k0HqHbYCyO48lAS2L5DmKUXPhH81ugVBbWCgX9EpzyLgUlloaTWnseadBr1rZBsUOUBqZ0M9gzJjtzYbqH8xvJiFtZ4Za01xveOJAv4qBp3uib9MHFtJqK1JkpQRP214aEQ)): 
  - données norvégiennes exhaustives
  - corrélation négative entre la taille des fratries et le nombre d'années de scolarisation moyenne des enfants

---
background-color: #fbe6ec
# Solution

### Sur `R`: se familiariser avec les données


Importer la [base de données]() `simulated_data_black_et_al_2005.csv` que vous nommerez `data`


```{r, echo=T, eval = T}
# Import data
data = read.csv2(here("Lecture 3/data", "simulated_data_black_et_al_2005.csv"), sep = ",") %>% 
  mutate(family_size = as.numeric(family_size),
         education = as.numeric(education),
         age_2000 = as.numeric(age_2000))
# data = read_sav(here("Lecture 3/data", "asciiqob.sav"))
```
 
---
background-color: #fbe6ec
# Solution


Calculer quelques statistiques descriptives de la **taille de la fratrie** (`family_size`) et du **nombre d'années d'études** (`education`): 
  - moyenne
  - écart-type
  - corrélation entre les deux variables

```{r, echo=T, eval = T}
data %>%
  summarise(
    across(c(family_size, education), ~ round(mean(.), 2), .names = "mean_{.col}"),
    across(c(family_size, education), ~ round(sd(.), 2), .names = "sd_{.col}"),
    correlation = cor(family_size, education, use = "complete.obs")
  )
```


---
background-color: #fbe6ec

# Solution 

Représenter le nuage de points qui définit la relation entre **taille de la fratrie** et **nombre d'années d'études**

```{r, echo=FALSE,fig.width = 8, fig.height = 5}
g_educ = ggplot(data, aes(x = as.numeric(family_size), y = as.numeric(education))) + 
  geom_point(size = 2, alpha = 0.5) +
  xlim(0,8) +
  ylim(0, 20) +
  labs(
    x = "Taille de la fratrie",
    y = "Nombre d'années d'études",
    title = "") +
  theme_minimum

g_educ
```



```{r, echo = F, eval = F}
# g_educ = ggplot(data, aes(x = education, y = as.numeric(log_weekly_wage))) + 
#   geom_point(size = 2, alpha = 0.5) +
#   xlim(0,20) +
#   ylim(-3, 15) +
#   labs(
#     x = "Highest grade completed",
#     y = "Log(weekly wage)",
#     title = "") +
#   theme_bw(base_size = 20)
# g_educ


```


```{r, echo=FALSE, include = F, fig.width = 8, fig.height = 5}
educ_avg_famsize <- data %>%
 mutate(family_size = as.numeric(family_size),
        education = as.numeric(education)) %>% 
 group_by(family_size) %>%
 summarise(mean_educ = mean(education))

g_educ_famsize = ggplot(educ_avg_famsize, aes(x = family_size, y = mean_educ)) + 
   geom_point(size = 2) +
   xlim(0,8) +
   ylim(0, 20) +
   labs(
     x = "Taille de la fratrie",
     y = "Nombre d'années d'études",
     title = "") +
   theme_minimum
g_educ_famsize


# wage_avg_grade <- data %>%
#  group_by(education) %>%
#  summarise(mean_wage = mean(log_weekly_wage))
# 
# g_wage_educ = ggplot(wage_avg_grade, aes(x = education, y = mean_wage)) + 
#    geom_point(size = 2) +
#    xlim(0,8) +
#    ylim(0, 20) +
#    labs(
#     x = "Highest grade completed",
#     y = "Log(weekly wage)",
#      title = "") +
#    theme_bw(base_size = 20)
# g_wage_educ


```



---
## Taille de fratrie et nombre d'années d'études

.middle[
```{r, echo = F,fig.width = 8, fig.height = 5}
plot_1 <- g_educ_famsize +
    ylim(11, 12.7) +
    theme_minimum

plot_1
```
]

---

## Taille de fratrie et nombre d'années d'études

.middle[
```{r, echo = F,fig.width = 8, fig.height = 5}
plot_2 <- plot_1 +
  stat_smooth(data = educ_avg_famsize, method = "lm", se = FALSE, colour = "#dd0747") +
  theme_minimum
plot_2
```
]


---
# 1. L'estimateur des MCO   

## 1.2. Du modèle univarié au modèle multivarié




$$\hat{\beta} = (X'X)^{-1}X'Y $$






---
## 5. Hypothèses de l'estimateur des MCO

- $H_1$ **Linéarité**: le modèle est linéaire dans les paramètres. 
  - Formellement, $\frac{\partial y_i}{\partial x_{ik}} = \beta_k$ $\forall k =1, ...; K$

--

- $H_2$ **X est exogène**: formellement, $\mathbb{E}(\varepsilon|X) = 0$ 

--

- $H_3$ : Il y a suffisamment de variation dans X. 
  - Dit autrement, chaque variable explicative apporte une information qui lui est propre
  - Formellement, s'il y a plusieurs variables explicatives, elles ne sont pas colinéaires, donc $(X'X)$ est inversible. 

--

- $H_4$ : **Homoscédasticité** : $\varepsilon_i$ est _iid_ et de distribution $\mathcal{N}(0, \sigma^2)$ 



---
## Remise en cause de $H_1$

**Quartet d'Anscombe**


```{r, echo = F, fig.width = 12, fig.height = 2}
anscombe = datasets::anscombe

p1 <- ggplot(anscombe, aes(x = x1, y = y1)) + geom_point() + stat_smooth(method = "lm", se = FALSE, colour = "#dd0747", fullrange = TRUE, linewidth = 0.5) + xlim(c(0, 20)) + ylim(c(0, 15)) + xlab("") + ylab("") + theme_minimum
p2 <- ggplot(anscombe, aes(x = x2, y = y2)) + geom_point() + stat_smooth(method = "lm", se = FALSE, colour = "#dd0747", fullrange = TRUE, linewidth = 0.5) + xlim(c(0, 20)) + ylim(c(0, 15)) + xlab("") + ylab("") + theme_minimum
p3 <- ggplot(anscombe, aes(x = x3, y = y3)) + geom_point() + stat_smooth(method = "lm", se = FALSE, colour = "#dd0747", fullrange = TRUE, linewidth = 0.5) + xlim(c(0, 20)) + ylim(c(0, 15)) + xlab("") + ylab("") + theme_minimum
p4 <- ggplot(anscombe, aes(x = x4, y = y4)) + geom_point() + stat_smooth(method = "lm", se = FALSE, colour = "#dd0747", fullrange = TRUE, linewidth = 0.5) + xlim(c(0, 20)) + ylim(c(0, 15)) + xlab("") + ylab("") + theme_minimum

grid.arrange(p1, p2, p3, p4, ncol = 4)
```





---
## Remise en cause de $H_1$

**Quartet d'Anscombe**


```{r, echo = F, fig.width = 12, fig.height = 2}

p1 <- ggplot(anscombe, aes(x = x1, y = y1)) + 
  geom_point() + 
  stat_smooth(method = "lm", se = FALSE, colour = "#dd0747", fullrange = TRUE, linewidth = 0.5) + 
  xlim(c(0, 20)) + ylim(c(0, 15)) + xlab("") + ylab("") + 
  theme_minimum

```


---
## Relation non-linéaire

```{r, echo = F, fig.width = 12, fig.height = 2}

p2 <- ggplot(anscombe, aes(x = x2, y = y2)) + 
  geom_point() + 
  stat_smooth(method = "lm", se = FALSE, colour = "#dd0747", fullrange = TRUE, linewidth = 0.5) + 
  stat_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, colour = "blue", fullrange = TRUE, linewidth = 0.5) + 
  xlim(c(0, 20)) + ylim(c(0, 15)) + xlab("") + ylab("") + 
  theme_minimum


```


---
## Outliers

```{r, echo = F, fig.width = 12, fig.height = 2}

p3 <- ggplot(anscombe, aes(x = x3, y = y3)) + 
  geom_point() + 
  stat_smooth(method = "lm", se = FALSE, colour = "#dd0747", fullrange = TRUE, linewidth = 0.5) + 
  xlim(c(0, 20)) + ylim(c(0, 15)) + xlab("") + ylab("") + 
  theme_minimum

p4 <- ggplot(anscombe, aes(x = x4, y = y4)) + 
  geom_point() + 
  stat_smooth(method = "lm", se = FALSE, colour = "#dd0747", fullrange = TRUE, linewidth = 0.5) + 
  xlim(c(0, 20)) + ylim(c(0, 15)) + xlab("") + ylab("") + 
  theme_minimum

grid.arrange(p3, p4, ncol = 4)
```


---
### Identifier les outliers

.pull-left[
```{R, outlier, echo = F, dev = "png", dpi = 300, cache = T}
df_with_outlier <- rbind(df %>% select(x,y,selected), data.frame(x = 10, y = 40, selected = "Outlier"))

ggplot(data = df_with_outlier, aes(x = x, y = y)) +
  geom_point(size = 3, alpha = 0.8) +
  geom_smooth(data = df_with_outlier, aes(x = x, y = y), method = "lm", color = "#921539", se = FALSE, size = 1) +
  geom_smooth(data = df, aes(x = x, y = y), method = "lm", color = "#dd0747", se = FALSE, size = 1, alpha = 0.5) +
  theme_empty

```
]

--

.pull-right[

**Identifier les outliers**:

- à partir de l'**écart-type**: *lorsque la distribution des données est relativement symmetrique*. Une observation éloignée de plus de 3 écarts-types de la moyenne peut être considérée comme une valeur abberrante 

- à partir de l'**écart interquartile**: peut être considérée comme outlier toute observation non incluse dans l'intervalle $[Q_1 - k(Q_3 - Q_1) \;;\; Q3 + k (Q3 - Q1)]$  où $k>0$. On détecte des outliers *moyens* pour $k=1.5$, et *extrêmes* pour $k=3$
]

---
## Remise en cause de $H_2$


---
## Remise en cause de $H_3$


---
## Remise en cause de $H_4$

---
# Références

- [Florian Oswald](https://raw.githack.com/ScPoEcon/ScPoEconometrics-Slides/master/chapter_slr/chapter_slr.html)
- [Edward Rubin](https://github.com/edrubin/EC421S19/tree/master)
- [Scott Cunningham](https://mixtape.scunning.com)
- [Scientific Research and Methodology, Peter K. Dunn](https://bookdown.org/pkaldunn/Book/)
- 

---
class: center, middle

# Annexe


---
name: derivation

## Calcul de l'estimateur des MCO dans le cas univarié

On a : $\text{SCE} = \sum_{i = 1}^N \varepsilon_i^2 = \sum_{i = 1}^N \left( y_i - \hat{y}_i\right)^2  = \sum_{i = 1}^N \left( y_i^2 - 2 y_i \hat{\alpha} - 2 y_i \hat{\beta} x_i + \hat{\alpha}^2 + 2 \hat{\alpha} \hat{\beta} x_i + \hat{\beta}^2 x_i^2 \right)$

Les conditions de premier ordre de la minimisation sont:

.center[
$\dfrac{\partial \text{SSE}}{\partial \hat{\alpha}} = 0$   **(1)**      et      $\dfrac{\partial \text{SSE}}{\partial \hat{\beta}} = 0$    **(2)**
]

Pour (1):

$$\begin{align} \dfrac{\partial \text{SSE}}{\partial \hat{\alpha}} = 0 
 \;\;\;\; &\implies \sum_{i = 1}^N \left( 2 \hat{\alpha} + 2 \hat{\beta} x_i - 2 y_i \right) = 2n \hat{\alpha} + 2 \hat{\beta} \sum_{i = 1}^N x_i - 2 \sum_{i = 1}^N y_i = 2n \hat{\alpha}2n \hat{\beta} \overline{x} - 2n \overline{y} = \; 0 \\&\implies \color{#dd0747}{\hat{\alpha} = \overline{y} - \hat{\beta} \overline{x}} \;\;\;\;\;(3)\end{align}$$
 
où 
$\overline{x} = \frac{\sum_{i=1}^N x_i}{n}$ et $\overline{y} = \frac{\sum_{i=1}^N y_i}{n}$ sont les moyennes de $x$ et $y$ sur notre échantillon de taille $n$.


---

Pour (2):

$$\begin{align} 
\dfrac{\partial \text{SSE}}{\partial \hat{\beta}} = 0  \;\;\;\; &\implies \sum_{i = 1}^N \left( 2 \hat{\alpha} x_i + 2 \hat{\beta} x_i^2 - 2 y_i x_i \right) = 2n \hat{\alpha} \overline{x} + 2 \hat{\beta} \sum_{i = 1}^N x_i^2 - 2 \sum_{i = 1}^N y_i x_i = 0   \;\;\;\;\; (4)\end{align}$$


En remplaçant $\hat{\alpha}$ par sa valeur définie dans (3), on obtient:
.center[
$2n \left(\overline{y} - \hat{\beta} \overline{x}\right) \overline{x} + 2 \hat{\beta} \sum_{i = 1}^N  x_i^2 - 2 \sum_{i = 1}^N  y_i x_i = 0$
]

en développant, 
.center[
$$2n \overline{y} \, \overline{x} - 2n \hat{\beta} \overline{x}^2 + 2 \hat{\beta} \sum_{i = 1}^N  x_i^2 - 2 \sum_{i = 1}^N  y_i x_i = 0 \; \implies \; 2 \hat{\beta} \left( \sum_{i = 1}^N  x_i^2 - n \overline{x}^2 \right) = 2 \sum_{i = 1}^N  y_i x_i - 2n \overline{y}\,\overline{x}$$

$$\implies \color{#dd0747}{\hat{\beta}} = \dfrac{\sum_{i = 1}^N  y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_{i = 1}^N  x_i^2 - n \overline{x}^2} = \dfrac{\sum_{i = 1}^N  (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i = 1}^N  (x_i - \overline{x})^2}   \color{#dd0747}{=\dfrac{Cov(x,y)}{Var(x)}}$$
]

[Back](#OLS)
